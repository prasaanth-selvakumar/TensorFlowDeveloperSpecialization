{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HorseOrHumanClassifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99kDSdyjzLG4"
      },
      "source": [
        "## Horses vs Humans Classifier \n",
        "\n",
        "Building the notebook from scratch \n",
        "  - using colab for this assigment because training the model without the GPU take a painfully long amount of time \n",
        "\n",
        "### Extracting data \n",
        "  Pull data from the data repository\n",
        "  Extract the data \n",
        "\n",
        "\n",
        "### Split it train and validation \n",
        "  Split the data into train and validation sets, also have these in directories so that the generator can be leveraged directly \n",
        "\n",
        "\n",
        "### Create Generators\n",
        "  After spliting the data we can create Generators for both train and validation \n",
        "\n",
        "\n",
        "### Define the model Architecture \n",
        "  Create a model architecture combinig convolution and Dense Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbcYS1Djxibk"
      },
      "source": [
        "!wget https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8oGBeiCx66g"
      },
      "source": [
        "!unzip horse-or-human.zip\n",
        "!mkdir all_data\n",
        "!cp -r horses/ ./all_data/horses\n",
        "!cp -r humans/ ./all_data/humans\n",
        "!rm -r horses\n",
        "!rm -r humans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gloj0kiGyXv-"
      },
      "source": [
        "# For spliting the files, let's use os.listdir to get the list of files \n",
        "# planning ot create an 80 20 split so using numpy for that \n",
        "# and shutil to copy the files \n",
        "# Start by checking the number of files \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAfIYImx1Efs"
      },
      "source": [
        "import os \n",
        "import numpy as np \n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inz7iT2c1Muw"
      },
      "source": [
        "humans = os.listdir('./all_data/humans/')\n",
        "print('# of humans in the data: ',len(humans))\n",
        "horses = os.listdir('./all_data/horses/')\n",
        "print('# of horses in the data: ',len(horses))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMB501O1k37"
      },
      "source": [
        "### There are approximately 500 images so we can use 400 for training and the remaing for testing \n",
        "\n",
        "human_prob = (np.random.rand(len(humans))>0.2)\n",
        "horse_prob = (np.random.rand(len(horses))>0.2)\n",
        "print(\"# of Humans in train: \",sum(human_prob))\n",
        "print(\"# of Horses in train: \",sum(horse_prob))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ofu8v1u2jwG"
      },
      "source": [
        "# Creating directories to be used by Humans \n",
        "\n",
        "os.mkdir('all_data/train/')\n",
        "os.mkdir('all_data/validate/')\n",
        "os.mkdir('all_data/train/humans/')\n",
        "os.mkdir('all_data/train/horses/')\n",
        "os.mkdir('all_data/validate/humans/')\n",
        "os.mkdir('all_data/validate/horses/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn8uLgmq25Os"
      },
      "source": [
        "# Splitting the data as train and test\n",
        "for index,status in enumerate(human_prob):\n",
        "  if status:\n",
        "    shutil.copy('./all_data/humans/'+humans[index],'./all_data/train/humans/'+humans[index])\n",
        "  else:\n",
        "    shutil.copy('./all_data/humans/'+humans[index],'./all_data/validate/humans/'+humans[index])\n",
        "\n",
        "for index,status in enumerate(horse_prob):\n",
        "  if status:\n",
        "    shutil.copy('./all_data/horses/'+horses[index],'./all_data/train/humans/'+horses[index])\n",
        "  else:\n",
        "    shutil.copy('./all_data/horses/'+horses[index],'./all_data/validate/humans/'+horses[index])\n",
        "\n",
        "## Files are copied"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJi3lrxm4RDo"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZQRJfU34zkU"
      },
      "source": [
        "train_gen = ImageDataGenerator(rescale=1./255)\n",
        "validate_gen = ImageDataGenerator(rescale=1./255)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocn9O8By-oDi"
      },
      "source": [
        "train_data = train_gen.flow_from_directory('all_data/train', #  Specifying the path of the directory that contains both the classes\n",
        "                                           target_size=(300,300),\n",
        "                                           batch_size=64,\n",
        "                                           class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhBbI0IE_UXo"
      },
      "source": [
        "validate_data = validate_gen.flow_from_directory('all_data/validate', #  Specifying the path of the directory that contains both the classes\n",
        "                                           target_size=(300,300),\n",
        "                                           batch_size=16,\n",
        "                                           class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g0ktyE6_siH"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqTb9n2jAoR-"
      },
      "source": [
        "hor_hum_classifier = keras.Sequential([keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(300,300,3)),\n",
        "                                       keras.layers.MaxPooling2D(2,2),\n",
        "                                       keras.layers.Conv2D(32,(3,3),activation='relu'),\n",
        "                                       keras.layers.MaxPooling2D(2,2),\n",
        "                                       keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "                                       keras.layers.MaxPooling2D(2,2),\n",
        "                                       keras.layers.Flatten(),\n",
        "                                       keras.layers.Dense(512,activation='relu'),\n",
        "                                       keras.layers.Dense(1,activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHGcxwi_BPHb"
      },
      "source": [
        "hor_hum_classifier.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBDzI4QXBgWF"
      },
      "source": [
        "hor_hum_classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl9dBmZTCClU"
      },
      "source": [
        "hor_hum_classifier.fit_generator(train_data, steps_per_epoch=13,\n",
        "                                 validation_data=validate_data, epochs= 20,\n",
        "                                 validation_steps= 4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RliecocVC0xv"
      },
      "source": [
        "## We have a classifier with almost 100 % accuracy to test it on original data use the image class from keras preprcessing"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odLq_Y0NDbR9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}